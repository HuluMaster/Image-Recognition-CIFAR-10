# -*- coding: utf-8 -*-
"""[Final]CNN ML Neural Network Imagine Recognition CIFAR_10(David)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/194NbDwyA74nzpk5rymKLnT7GJ3ubMXxy
"""

###DO NOT EDIT THIS CODE
################################################################################################################################

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torchvision import datasets, transforms
from PIL import Image
import matplotlib.pyplot as plt

# GPUs are 3x faster than CPU. Better to use if it is available 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define Loss Function
loss_function = nn.CrossEntropyLoss()

# This function returns the number of parameters in the model
def num_params(model):
  return sum([p.numel() for p in model.parameters()])

# Define a Training Function. This function will: compute the forward pass, backpropagate,
# update the weights, and repeat the steps for a given number of epochs. At each epoch, 
# it will output the training loss and test loss at every step
def train(epochs, model, trainloader, testloader, optimizer, loss_function):
  for epoch in range(epochs):
    loss_epoch = np.array([])
    train_correct, train_total = 0, 0
    test_correct, test_total = 0, 0

    for data, labels in trainloader:
      # convert into GPU objects if needed
      input_data = data.to(device)
      labels = labels.to(device)

      # forward pass
      predict = model(input_data)
      
      # backward pass
      loss = loss_function(predict, labels)
      optimizer.zero_grad()
      loss.backward()

      # update parameters (weights and biases)
      optimizer.step()

      # store progress
      loss_epoch = np.append(loss_epoch, loss.item())

    # evaluate test accuracy
    for data, labels in testloader:
      input_data = data.to(device)
      labels = labels.to(device)
      predict = model(input_data)
      for i, out in enumerate(predict):
        pred = torch.argmax(out)
        if pred == labels[i]:
          test_correct+=1
        test_total+=1

    test_accuracy = test_correct/test_total    
  
    print('epoch [{}/{}], training loss:{:.4f}, test accuracy:{:.4f}'.format(epoch+1, epochs, np.mean(loss_epoch), test_accuracy))
################################################################################################################################

"""# **Load Dataset**

Available datasets are: MNIST, CIFAR10

## CIFAR10
"""

# download and load data
batch_size = 512

# download and transform train dataset
train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./cifar10_data', download=True, train=True, transform=transforms.Compose([
                                                transforms.ToTensor(), # first, convert image to PyTorch tensor
                                                transforms.Normalize((0.1307,), (0.3081,)),
                                                transforms.Grayscale() # normalize inputs
                                                ])), batch_size=batch_size, shuffle=True)

# download and transform test dataset
test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./cifar10_data', download=True, train=True, transform=transforms.Compose([
                                                transforms.ToTensor(), # first, convert image to PyTorch tensor
                                                transforms.Normalize((0.1307,), (0.3081,)),
                                                transforms.Grayscale() # normalize inputs
                                                ])), batch_size=batch_size, shuffle=True)

# it is a good idea to take a look at the data. Here we see it is a 28x28 grayscale image
for data, labels in train_loader:
  print(data[0].size())
  break

"""# **Build a Network and Define Hyperparameters**"""

12########################
####                #### 
#### EDIT THIS CELL ####
####                ####
########################

learning_rate = 10e-4
weight_decay = 10e-5
n_epochs = 25

# neural network
class NeuralNetwork(nn.Module):

  def __init__(self):

    super(NeuralNetwork, self).__init__()

    ### Define Layers
    self.conv1 = nn.Conv2d(1, 1, 3,padding=1)
    self.fc1 = nn.Linear(1024,700)
    self.fc2 = nn.Linear(700, 300)
    self.fc3 = nn.Linear(300,10)

  def forward(self, x):

    x = self.conv1(x)
    #x = F.BatchNorm2d(x)
    x = self.conv1(x)
    x = torch.flatten(x, 1)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = (self.fc3(x))
    return x


# Every time you edit the neural network, you'll have to update this cell
# Create model object
model = NeuralNetwork().to(device)


# Loads Adam optimizer, which implements a version of gradient descent
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

from PIL import Image

for data, labels in test_loader:
  exd = data[1]
  exl = labels[1]
  break
print(model.conv1(exd.to(device)))

import matplotlib.pyplot as plt
plt.imshow(exd.permute(1,2,0).squeeze(), cmap='gray')
ex2 = (model.conv1(exd.to(device))).cpu().detach()
ex3 = (model.conv1(ex2.to(device))).cpu().detach()

plt.imshow(ex2.permute(1,2,0).squeeze(), cmap='gray')

plt.imshow(ex3.permute(1,2,0).squeeze(), cmap='gray')

# check the structure of your network
print(model)

# apply your model to a single input. This helps check that 
# the dimensions are correct
model(torch.rand(1,1,32,32, device=device))

"""# **Train and Validate**"""

train(n_epochs, model, train_loader, test_loader, optimizer, loss_function)

